<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Social-Media-Users</title>
    <link rel="stylesheet" href="../../styles.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/flat-ui/2.2.2/css/flat-ui.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css">
</head>

<body>
    <nav class="navbar navbar-inverse navbar-fixed-top transparent">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar"
                    aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                </button>
                <a class="navbar-brand" href="../../index.html">Team # 2 AI Portfolio</a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="#about">About</a></li>
                    <li><a href="#portfolio">Portfolio</a></li>
                    <li><a href="#contact">Contact</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <main data-bs-spy="scroll" data-bs-target=".navbar" data-bs-offset="50">
        <div class="container">
            <div class="row mb-5">
                <div class="col-md-12">
                    <h4 style="text-align: center;">Segment social media users</h4>
                    <div class="d-flex justify-content-center align-items-center">
                    </div>
                    <p><strong>Introduction:</strong><br>
                        Based on the Kaggle dataset available in <a
                            href="https://www.kaggle.com/datasets/arindamsahoo/social-media-users"
                            target="_blank">Social-Media-Users-Dataset</a>, this project aims to develop a methodology to segment social media users based on their common interests. 
                        Segmentation will allow suggesting connections between users with similar affinities, facilitating the creation of more meaningful communities and enhancing the user experience. 
                        Interest-based segmentation not only increases the likelihood of successful connections but also optimizes recommended content based on what users value and share in common. 
                        This dataset contains 10,000 rows, which makes it ideal for this analysis.
                    </p>
                    <p><strong>Why did I choose this case study?</strong><br>
                        I chose this case study because user segmentation in social media is a highly relevant topic in today's digital world. 
                        In an environment where platforms compete to attract and retain users, offering personalized experiences has become a key strategy. 
                        Grouping users based on common interests not only improves friend and content recommendations but also fosters meaningful interactions and the creation of virtual communities that enrich the user experience.
                    </p>
                    <p><strong>Objectives:</strong><br>
                        The focus of this project is to group users based on their common interests by identifying groups of users with similar interests through an unsupervised learning algorithm.
                    </p>
                    <h5>Development</h5>
                    <h6>Analysis of attributes and their distributions:</h6>
                    <ul>
                    
                        <li><strong>UserID:</strong> User ID.<br>
                            It has a uniform distribution (each user has a unique ID with no repeated values).
                        </li>
                    
                        <li><strong>Name:</strong> User's name.<br>
                            No null or empty values were identified.
                        </li>
                    
                        <li><strong>Gender:</strong> Represents the users' gender: "Male" and "Female".<br>
                            The proportion of "Male" and "Female" is close to 50-50, indicating a nearly balanced binomial distribution. 
                            The distribution is slightly skewed toward "Female," though the difference is minimal.
                        </li>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/image.png" class="img-fluid" style="width: 30%;">
                        </div>
                        <li><strong>DOB:</strong> Users' date of birth.<br>
                            Based on this attribute, we could calculate the age of each user if needed.
                            When calculating users' ages in a specific year (e.g., 2024), the ages would range from 20 years (born in 2004) to 70 years (born in 1954).
                            The ages follow a right-skewed distribution, meaning there are more young users aged between 20 and 40 than users older than 50.
                        </li>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/imageDOB.png" class="img-fluid" style="width: 30%;">
                        </div>
                    
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/imageDob2.png" class="img-fluid" style="width: 30%;">
                        </div>
                    
                        <li><strong>Interests:</strong> List of interests per user.<br>
                            The distribution of interests in this dataset provides key insights into the variety and frequency of interests among social media users. 
                            This analysis is crucial for understanding users' general preferences and how these interests can be grouped to generate personalized recommendations.
                            Interests are represented as a polynomial attribute, where each user may have one or more associated interest tags.
                    
                            Some users have a single interest, while others have multiple. This can be analyzed by grouping users based on the number of listed interests.
                    
                            It is possible to count how many times each interest or set of interests appears in the dataset. For example: "Fashion": 142 occurrences.
                        </li>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/imageInterest.png" class="img-fluid" style="width: 30%;">
                        </div>
                    
                        <li><strong>City:</strong> City where each user resides.<br>
                            The distribution of cities is highly skewed, as most cities have only one registered user, with only a few cities having more than one user, which 
                            represents a wide population dispersion.
                        </li>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/city.png" class="img-fluid" style="width: 30%;">
                        </div>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/city2.png" class="img-fluid" style="width: 30%;">
                        </div>
                    
                        <li><strong>Country:</strong> Country where the user resides.<br>
                            The distribution is positively skewed since a small group of countries accounts for the majority of records.
                            The tail of the distribution shows many countries with few records, which is common in datasets of this type where certain locations are more prominent.
                            No clear outliers were identified in the distribution. Records from less frequent countries are consistent with the nature of the dataset.
                        </li>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/country.png" class="img-fluid" style="width: 30%;">
                        </div>
                    </ul>

                    <h6>Identification of the Problem Type:</h6>
                    <p>
                        In this case, the objective is to group users based on their interests using the Social Media Users dataset. The problem is of the unsupervised learning type: Clustering. 
                        This is because there are no labels or predefined target variables (e.g., "user group") provided. This means the model needs to identify patterns or groups solely based on the available data. 
                        Clustering is a technique for uncovering hidden groups or patterns in data, such as common interests in this case.
                        For this type of problem, the K-Means algorithm can be used because it groups users by minimizing the distances within clusters and maximizing the distances between them. 
                        This is useful for identifying common patterns among users with similar interests. This algorithm also allows us to experiment with different values of 
                        k (number of clusters) and select the optimal one to achieve the desired adjustment.
                    </p>

                    <h5>K-Means Model Application</h5>
                    <p>Process created in RapidMiner using the K-Means algorithm to segment social media users based on their common interests.</p>
                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/allProcess.png" class="img-fluid" style="width: 60%;">
                    </div>
                    
                    <p>Sub-process within "Loop Parameters".</p>
                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/subProcess.png" class="img-fluid" style="width: 60%;">
                    </div>
                    
                    <p>1. Sample:  
                        Its purpose is to extract a sample of data from the original dataset.  
                        This operator is generally used to handle a more manageable subset of data when the original dataset is very large, enabling quick testing and avoiding performance issues.
                    </p>
                    
                    <p>2. Remove Duplicates:  
                        Its purpose is to remove duplicate records from the dataset.  
                        Duplicates can bias the clustering results by making certain clusters appear denser than they actually are.
                    </p>
                    
                    <p>3. Date to Numerical:  
                        The purpose is to convert date-type attributes (such as DOB) into numerical values.  
                        Reason: Clustering algorithms cannot directly handle date-type data. In this case, it was used to calculate the age from the year of birth.
                    </p>
                    
                    <p>4. Generate Attributes:  
                        The purpose is to create derived attributes based on calculations.  
                        This block was used to calculate the user's age from the date of birth (DOB), transforming it into a numerical attribute relevant for clustering.
                    </p>
                    
                    <p>5. Select Attributes:  
                        The purpose is to select the relevant attributes for the analysis.  
                        Reason: Clustering works better with meaningful attributes, so we selected Age, Interests, and Gender as the most relevant for segmentation.
                    </p>
                    
                    <p>6. Nominal to Numerical:  
                        The purpose is to convert categorical (nominal) attributes into a numerical format.  
                        K-Means requires all attributes to be numerical in order to calculate distances between points.
                    </p>
                    
                    <p>7. Normalize:  
                        The purpose is to scale the numerical attributes to a common range (e.g., [0,1] or z-standardized values).  
                        K-Means is sensitive to the scale of attributes. Normalization ensures that no attribute dominates the distance calculation due to a broader range of values.
                    </p>
                    
                    <p>8. Loop Parameters:  
                        The purpose is to apply and optimize the clustering algorithm.  
                        This operator is used to perform an iterative analysis with different configurations of the algorithm, such as the number of clusters k.  
                        It sets the range of values for k and evaluates the performance of each model based on metrics such as the elbow method or cluster distance.
                    </p>
                    
                    <p>9. K-Means Clustering:  
                        The purpose is to group the data into k clusters based on similarities among the selected attributes.  
                        This step generates the final clusters and assigns each record to a cluster.  
                        The optimal number of clusters (k) must be determined based on previous tests. The Euclidean distance is used to measure similarity between points.
                    </p>


                    <h5>Selection of the k Parameter in K-Means</h5>
                    <p>
                        The Loop Parameters operator is key to performing iterative experiments with different parameter configurations. In the case of the K-Means algorithm, 
                        this operator allows testing different values of k to determine the most suitable one.
                    
                        In this case, we used the method known as the "Elbow Method." It is a visual technique used to determine the optimal number of clusters (k) in clustering algorithms.
                    
                        To perform this method, we must configure the Cluster Distance Performance operator to calculate the metric "Avg. within centroid distance" in each iteration.
                    
                        The x-axis represents the number of clusters k, and the y-axis represents the metric "Avg. within centroid distance" calculated for each k.
                    
                        In this case, we adjusted the Loop Parameters operator to explore values from k = 2 to k = 30 with 20 steps.
                    </p>
                    
                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/elbow.png" class="img-fluid" style="width: 60%;">
                    </div>
                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/elbowAvg.png" class="img-fluid" style="width: 60%;">
                    </div>
                    
                    <p>
                        Once the graphical representation was created, we can observe that the graph follows a decreasing trend, as expected. As the number of clusters increases, 
                        the average distance to the centroid decreases because there are more clusters to capture the variations within the data.
                    
                        The "elbow" is the point at which the reduction in the average distance starts to become less significant. In this case, the elbow appears to be around k = 12. This means that up to 
                        k = 12, adding more clusters results in a substantial improvement in cluster compactness. Beyond k = 12, the improvement is marginal.
                    
                        Choosing k = 12 balances a good segmentation of the data without overfitting.
                    </p>
                    
                    <p>
                        We can also calculate the most efficient k using the Davies-Bouldin Index, which represents the relationship between cluster compactness and separation.
                        In this case, we need to find the point where the index is lowest, as this indicates the best k.
                        With the same parameters as the previous method, we obtain the following results.
                    </p>
                    
                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/daviesBoudinK.png" class="img-fluid" style="width: 60%;">
                    </div>

                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/daviesBoudin.png" class="img-fluid" style="width: 60%;">
                    </div>

                    <h5>Algorithm Parameter Selection</h5>
                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/kmeansParameters.png" class="img-fluid" style="width: 60%;">
                    </div>
                    
                    <p>
                        We can observe that choosing k = 12 is an optimal value according to this method to avoid overfitting the algorithm, which aligns with the index obtained from the "Avg. within centroid distance" metric.
                    </p>

                    <p>
                        Additionally, selecting a maximum number of iterations ensures that the algorithm does not remain stuck in an infinite loop while trying to optimize the clusters. A reasonable value like 10 is sufficient for the algorithm to explore several initializations and find a good point of convergence.
                    </p>

                    <p>
                        The parameter measure types = BregmanDivergences was selected because this metric is consistent with the type of data being processed. Bregman divergences are a suitable mathematical extension when working with transformed and normalized data, as in this case.
                    </p>

                    <p>
                        The parameter divergence = SquaredEuclideanDistance was chosen because it is a standard metric in K-Means, effectively measuring the differences between points in a multidimensional space. This is appropriate because the interests and characteristics (such as age and gender) have been uniformly scaled, eliminating scale biases.
                    </p>

                    <p>
                        Finally, the parameter max optimization steps was selected because this value ensures that K-Means has enough steps to optimize the position of the centroids and adjust the clusters. In this case, with complex data and a number of clusters (k=15), setting a sufficiently high limit such as 100 allows for proper adjustment without overburdening the processing.
                    </p>
                    
                    <h5>We executed the algorithm and obtained the following results</h5>
                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/clusters.png" class="img-fluid" style="width: 60%;">
                    </div>
                    
                    <p>The x-axis represents the "Interests" feature of the data, previously normalized, and the y-axis represents the "Age" feature, also normalized.  
                        Each color denotes a unique cluster, with a total of 12 clusters represented.  
                        The clusters appear to be reasonably distributed, with visible differentiation in certain regions. However, some clusters seem to overlap or have less clear separation.  
                        This is due to the nature of the data, as there may be people of different ages with the same interests or vice versa. These attributes are not completely independent or perfectly discriminative.
                    </p>
                    
                    <div class="d-flex justify-content-start align-items-start">
                        <img src="../../Assets/research_images/userInterestAssets/clustersGender.png" class="img-fluid" style="width: 60%;">
                    </div>
                    
                    <p>In this case, the x-axis represents the "Interests" feature of the data, previously normalized, and the y-axis represents the "Gender" feature.  
                        Each color denotes a unique cluster, with a total of 12 clusters represented.  
                        The clear separation into two horizontal lines (-1 and 1) indicates that the data is distributed solely between two genders, meaning the attribute has no intermediate values.  
                        It is also observed that the cluster colors are mixed within the Interests values, indicating that Interests is not a completely discriminative attribute for separating clusters. This reflects the nature of the data, where different people share similar interests regardless of gender.  
                        Some clusters occupy broad ranges on the Interests axis, suggesting greater diversity in interests within those groups.
                    </p>

                    <h5>Dataset Modification</h5>
                    <p>
                        Since the original dataset contained user interests grouped into a single column as a polynomial attribute (a list of multiple values separated by commas for each user), certain challenges arose when applying the K-Means algorithm. 
                        As a distance-based algorithm, K-Means calculates similarities between points based on numerical coordinates in a multidimensional space. However, when interests are grouped in a single column, this information is not translated effectively into a format that K-Means can interpret accurately.
                        Grouping interests into a single attribute complicates identifying specific patterns in the data, as it becomes difficult to precisely determine which interests users actually share. The algorithm may treat the concatenated values as a single entity, resulting in clusters based on incorrect or irrelevant similarities.
                        
                        To address these issues, a One-Hot Encoding approach was implemented. This method involves transforming the "Interests" attribute into a set of binary attributes, where each unique interest becomes an independent column. In these new columns:
                        
                        - "1" indicates that the user has that specific interest.
                        - "0" indicates that the user does not have that interest.
                        
                        By making this modification, several benefits were achieved:
                        <p>
                        Better interpretation by K-Means: By separating interests into binary attributes, each becomes a dimension in the space where K-Means operates, enabling accurate distance calculations between points.
                        </p>

                        <p>
                        Preservation of granularity: It is now possible to analyze specific interest patterns for each user, improving the quality of the clusters.
                        </p>

                        <p>
                        Greater flexibility for future analysis: With interests separated into individual attributes, more detailed visualizations can be created, such as stacked bar charts or heatmaps, to observe how interests are distributed across clusters.
                        </p>

                        <p>
                        Optimization of recommendations: Separating interests allows for more effectively identifying groups of users with shared preferences, facilitating personalized recommendations and the creation of communities based on affinities.
                        </p>
                    </p>
                        <h5>The dataset was modified using one-hot encoding.</h5>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/one-hot-encoding.png" class="img-fluid" style="width: 60%;">
                        </div>

                        <h5>The value of k was recalculated to determine the most accurate one possible using the "Elbow Method."</h5>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/elbow-One-Hot-Encoding.png" class="img-fluid" style="width: 60%;">
                        </div>

                        <p>In this case, a good value for k could be 15, as adding more clusters does not significantly improve segmentation. 
                            This behavior is typical of the "elbow" in the graph, making k=15 a solid choice. If k is too high, there is a risk of creating small 
                            and overly specific clusters that do not generalize well.
                        </p>

    
                        <h5>After selecting k and executing the algorithm under the same conditions, we obtained the following results: </h5>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/barInterestAgeNormalized.png" class="img-fluid" style="width: 60%;">
                        </div>
                        
                        <p>By separating the interests into different columns, the clusters appear more clearly defined, allowing us to select only the interests we want to analyze.
                            In this case, we selected two interests in relation to the denormalized age to provide an example of how we can uncover different patterns of relevant information within the data.
                            This chart shows how the data is distributed based on denormalized age, with values separated into individual columns. This approach of separating interests into columns facilitates a more precise and segmented analysis of the data.
                            The interests "Art" (represented in blue) and "Books" (represented in green) were used. Each bar indicates the total number of users with a specific interest within each age range. The X-axis shows the actual ages of the users, ranging from 20 to 70 years. This change from normalized to real values allows for a direct interpretation of the information without abstract scales.
                            For younger ages (20-30 years), both interests show a stable distribution, but "Books" has a higher number of users. As age increases, the difference between the two interests remains relatively constant, although there may be slight variations.
                            This type of visualization helps identify user segments based on their interests and ages. It also enables personalized content or product recommendations for specific age groups.
                    </p>

                    <h5>Discovering More Information About the Data</h5>
                        <div class="d-flex justify-content-start align-items-start">
                            <img src="../../Assets/research_images/userInterestAssets/interestGaming.png" class="img-fluid" style="width: 60%;">
                        </div>
                        
                        <p>
                            This example reflects the gender proportion for the interest "Gaming" in each cluster. The chart shows how each cluster is composed in terms of gender for this specific interest.
                            In the case of cluster 9, we can see that it is entirely composed of gender "0" (green), indicating that all users in this cluster who are interested in gaming belong to this gender.
                            Other clusters, such as cluster 11, show a combination of both genders, with specific proportions of blue (gender "1") and green (gender "0").
                            If certain clusters have a predominant gender for an interest, this could indicate a strong correlation between that gender and the interest. In clusters with both genders present, it would be possible to analyze whether the interest is equally appealing to all or if there are differences in the proportion of users.
                            This analysis can help segment users by affinity and personalize recommendations or strategies to attract different groups based on their gender and interests.
                        </p>
                    
                </div>
            </div>
        </div>

        <div class="container mt-5">
            <h4 class="mb-4">Conclusion</h4>
            <p>
                This case study demonstrated how the K-Means algorithm can be used to segment social media users based on their common interests and other key characteristics, such as age and gender. Effective segmentation is essential for personalizing user experiences and optimizing recommendations, which significantly enhances user interaction and satisfaction.
            </p>
            
            <p>
                To determine the optimal number of groups, two main metrics were used: the average within centroid distance (Elbow Method) and the Davies-Bouldin Index. These tools allowed us to establish the most suitable value for k, ensuring balanced and representative segmentation. 
                It is important to note that the K-Means algorithm is sensitive to the selection of the k parameter, which can influence the quality of the generated groups. Additionally, K-Means tends to create groups of uniform size, which does not always accurately reflect the actual distribution of the data.
            </p>
            
            <p>
                Despite these limitations, the results obtained provide valuable insights for recommending content and establishing connections between users with similar interests. 
                It is worth highlighting the importance of data preprocessing, as without applying the one-hot encoding technique to people's interests, the results obtained would have been very different and would have provided information that is not entirely accurate.
            </p>

            <p>
                Finally, after applying the algorithm, relevant patterns were identified, and new information was discovered that can be used for various purposes. Furthermore, all the information generated and represented in the clusters can be processed and denormalized to visualize specific details, such as which users have certain interests. This allows for a greater level of granularity, facilitating more precise and detailed classification.
            </p>
          
    <div class="text-center text-inverse navbar-inverse">
        <div class="container">
            <div class="row">
                <div class="footer-col col-md-4" id="contact">
                    <h3>Location</h3>
                    <p>Montevideo, Uruguay</p>
                </div>
                <div class="footer-col col-md-4">
                    <h3>Connect</h3>
                    <ul class="list-inline">
                        <li>
                            <!-- <a class="medium" href="https://github.com/Ionasjospo" target="newwindow"><span
                                    class="fui-github"></span></a> -->
                        </li>
                        <li>
                            <!-- <a class="medium" href="https://www.linkedin.com/in/ionas-josponis/"
                                target="newwindow"><span class="fui-linkedin"></span></a> -->
                        </li>
                    </ul>
                </div>
                <div class="footer-col col-md-4">
                    <h3>Hire Us</h3>
                    <p>We are available for workshops</p>
                </div>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-HoRfnjQe6LRgsZAWB7tF0/J9UvcrHJ/tDvkfjTcZ4IM3wb/SuRoGJb7nVLv9ej3M"
        crossorigin="anonymous"></script>
</body>

</html>